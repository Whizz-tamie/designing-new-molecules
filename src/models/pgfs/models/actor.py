# models/actor.py

import logging

import torch
import torch.nn as nn
import torch.nn.functional as F

import src.models.pgfs.logging_config as logging_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dims=[256, 128, 128]):
        logger.info("Actor FNetwork initialised...")
        super(FNetwork, self).__init__()
        layers = []
        for i in range(len(hidden_dims)):
            layers.append(
                nn.Linear(input_dim if i == 0 else hidden_dims[i - 1], hidden_dims[i])
            )
            layers.append(nn.ReLU())
            input_dim = hidden_dims[i]  # Update input dimension for the next layer
        layers.append(nn.Linear(hidden_dims[-1], output_dim))
        self.network = nn.Sequential(*layers)

        # Initialize weights using He initialization
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_uniform_(layer.weight, nonlinearity="relu")

    def forward(self, state):
        output = self.network(state)
        logger.debug(f"FNetwork output: {output}")
        return torch.tanh(output)


class PiNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dims=[256, 256, 167]):
        logger.info("Actor PiNetwork initialised...")
        super(PiNetwork, self).__init__()
        layers = []
        for i in range(len(hidden_dims)):
            layers.append(
                nn.Linear(input_dim if i == 0 else hidden_dims[i - 1], hidden_dims[i])
            )
            layers.append(nn.ReLU())
            input_dim = hidden_dims[i]  # Update input dimension for the next layer
        layers.append(nn.Linear(hidden_dims[-1], output_dim))
        self.network = nn.Sequential(*layers)

        # Initialize weights using He initialization
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_uniform_(layer.weight, nonlinearity="relu")

    def forward(self, combined_input):
        output = self.network(combined_input)
        logger.debug(f"PiNetwork output: {output}")
        return torch.tanh(output)


class ActorNetwork(nn.Module):
    def __init__(self, state_dim, template_dim, action_dim):
        super(ActorNetwork, self).__init__()
        logger.info(
            "ActorNetwork initialized with state_dim={}, template_dim={}, action_dim={}".format(
                state_dim, template_dim, action_dim
            )
        )
        self.f_net = FNetwork(state_dim, template_dim)
        self.pi_net = PiNetwork(state_dim + template_dim, action_dim)
        self.logits = None

    def forward(self, state, template_mask_info=None, temperature=1.0, evaluate=False):
        logits = self.f_net(state)
        self.logits = logits  # Save logits as an attribute
        logger.debug(f"Logits generated by FNetwork: {logits}")

        template_one_hot, template_types = self._apply_template_mask(
            logits, template_mask_info, temperature, evaluate
        )
        template_index = template_one_hot.argmax(dim=-1).item()
        logger.info(
            f"Selected template index: {template_index}, Evaluate mode: {evaluate}"
        )

        template_index = template_one_hot.argmax(dim=-1).item()

        # Determine if bimolecular action is necessary
        if template_types.get(template_index, "") == "bimolecular":
            combined_input = torch.cat((state, template_one_hot), dim=-1)
            r2_vector = self.pi_net(combined_input)
            logger.info(
                "Bimolecular reaction required, second reactant vector computed."
            )
        else:
            r2_vector = None
            logger.info("Unimolecular reaction or no second reactant required.")

        return template_one_hot, r2_vector

    def _apply_template_mask(self, logits, template_mask_info, temperature, evaluate):
        """Applies template masking logic to logits based on the template mask info."""
        if template_mask_info is None:
            if evaluate:
                return F.one_hot(logits.argmax(dim=-1), num_classes=logits.size(1)), {}
            else:
                return F.gumbel_softmax(logits, tau=temperature, hard=True), {}
        else:
            template_mask, template_types = template_mask_info
            masked_logits = logits + (1 - template_mask) * (-1e9)  # Apply mask
            if evaluate:
                return (
                    F.one_hot(
                        masked_logits.argmax(dim=-1), num_classes=masked_logits.size(1)
                    ),
                    template_types,
                )
            else:
                return (
                    F.gumbel_softmax(masked_logits, tau=temperature, hard=True),
                    template_types,
                )
